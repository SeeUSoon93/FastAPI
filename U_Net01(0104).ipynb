{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm",
      "mount_file_id": "1dFdQ86L_xZbFYvEZAjhAxYmEQxyzj8IH",
      "authorship_tag": "ABX9TyNHYpCJ9dXgPFsDkJDikPLA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeeUSoon93/FastAPI/blob/main/U_Net01(0104).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khYjN8uPk0Uk",
        "outputId": "14622828-6fd8-415f-cf14-3666591fdd9d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "355EP0SdlPOL",
        "outputId": "b7abf7b5-82db-4493-a3db-d75bf115fbb3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "rN0lhve_kWip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88513d25-6f13-464a-d6aa-d827cca9b872"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_folder = 'colab/data/test/dog'\n",
        "class_folders = os.listdir(base_folder)"
      ],
      "metadata": {
        "id": "nQHBksSukzL4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"image_paths = []\n",
        "json_paths = []\n",
        "\n",
        "for class_folder in class_folders:\n",
        "    image_files = glob.glob(os.path.join(base_folder, class_folder, '*.jpg'))\n",
        "    json_files = [file.replace('.jpg', '.json') for file in image_files]\n",
        "\n",
        "    image_paths.extend(image_files)\n",
        "    json_paths.extend(json_files)\n",
        "\n",
        "image_paths = [path.replace('\\\\', '/') for path in image_paths]\n",
        "json_paths = [path.replace('\\\\', '/') for path in json_paths]\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'image_path': image_paths,\n",
        "    'json_path': json_paths,\n",
        "    'label': [path.split('/')[-2] for path in image_paths]\n",
        "})\"\"\""
      ],
      "metadata": {
        "id": "oeQLxZ7PlqkA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"df.to_csv('colab/csv/df.csv', encoding='utf-8-sig', index=False)\"\"\""
      ],
      "metadata": {
        "id": "oJitLX_Hnmhw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"colab/csv/df.csv\")"
      ],
      "metadata": {
        "id": "82XzaVYULxwI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(image_path, json_path):\n",
        "    try:\n",
        "        with Image.open(image_path) as img:\n",
        "            original_size = img.size\n",
        "            img = img.resize((256, 256))\n",
        "            image = np.array(img, dtype=np.float32)\n",
        "\n",
        "        with open(json_path, 'r', encoding='UTF-8') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        mask = Image.new('L', original_size, 0)\n",
        "        draw = ImageDraw.Draw(mask)\n",
        "\n",
        "        for annotation in data['labelingInfo']:\n",
        "            if 'polygon' in annotation:\n",
        "                polygon = []\n",
        "                for i in range(1, len(annotation['polygon']['location'][0]) // 2 + 1):\n",
        "                    x_key = f'x{i}'\n",
        "                    y_key = f'y{i}'\n",
        "                    x = annotation['polygon']['location'][0].get(x_key)\n",
        "                    y = annotation['polygon']['location'][0].get(y_key)\n",
        "                    if x is not None and y is not None:\n",
        "                        polygon.append((x, y))\n",
        "\n",
        "                if polygon:\n",
        "                    draw.polygon(polygon, outline=1, fill=1)\n",
        "\n",
        "        mask = mask.resize((256, 256))\n",
        "        mask = np.array(mask, dtype=np.float32)\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "\n",
        "        return image, mask\n",
        "    except Exception as e:\n",
        "        empty_image = np.zeros((256, 256, 3), dtype=np.float32)\n",
        "        empty_mask = np.zeros((256, 256, 1), dtype=np.float32)\n",
        "        return empty_image, empty_mask"
      ],
      "metadata": {
        "id": "snL8Ww6IlxFt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_image_and_mask(image, mask, angle):\n",
        "    image = tfa.image.rotate(image, angle)\n",
        "    mask = tfa.image.rotate(mask, angle)\n",
        "    return image, mask\n",
        "\n",
        "def tf_create_mask(image_path, json_path):\n",
        "    [image, mask] = tf.numpy_function(create_mask, [image_path, json_path], [tf.float32, tf.float32])\n",
        "    image.set_shape([256, 256, 3])\n",
        "    mask.set_shape([256, 256, 1])\n",
        "    return image, mask\n",
        "\n",
        "image_paths = df['image_path'].values\n",
        "json_paths = df['json_path'].values\n",
        "\n",
        "original_dataset = tf.data.Dataset.from_tensor_slices((image_paths, json_paths))\n",
        "original_dataset = original_dataset.map(tf_create_mask, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "augmented_datasets = []\n",
        "for angle in [0, np.pi/2, np.pi, 3*np.pi/2]:  # 0, 90, 180, 270도\n",
        "    augmented_dataset = original_dataset.map(lambda x, y: rotate_image_and_mask(x, y, angle),\n",
        "                                             num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    augmented_datasets.append(augmented_dataset)\n",
        "\n",
        "# 모든 데이터셋을 결합합니다.\n",
        "dataset = augmented_datasets[0]\n",
        "for augmented_dataset in augmented_datasets[1:]:\n",
        "    dataset = dataset.concatenate(augmented_dataset)\n",
        "\n",
        "dataset_size = len(image_paths)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "batch_size = 64\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size=10000).cache().repeat()\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)\n",
        "\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "iFRRDEOplxKF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unet_model(input_size=(256, 256, 3),dropout_rate=0.5):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # 인코더\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    pool1 = Dropout(dropout_rate)(pool1)\n",
        "\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    pool2 = Dropout(dropout_rate)(pool2)\n",
        "\n",
        "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    pool3 = Dropout(dropout_rate)(pool3)\n",
        "\n",
        "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "    pool4 = Dropout(dropout_rate)(pool4)\n",
        "\n",
        "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "\n",
        "    # 디코더\n",
        "    up6 = UpSampling2D(size=(2, 2))(conv5)\n",
        "    merge6 = concatenate([conv4, up6], axis=3)\n",
        "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Dropout(dropout_rate)(conv6)\n",
        "\n",
        "    up7 = UpSampling2D(size=(2, 2))(conv6)\n",
        "    merge7 = concatenate([conv3, up7], axis=3)\n",
        "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Dropout(dropout_rate)(conv7)\n",
        "\n",
        "    up8 = UpSampling2D(size=(2, 2))(conv7)\n",
        "    merge8 = concatenate([conv2, up8], axis=3)\n",
        "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Dropout(dropout_rate)(conv8)\n",
        "\n",
        "    up9 = UpSampling2D(size=(2, 2))(conv8)\n",
        "    merge9 = concatenate([conv1, up9], axis=3)\n",
        "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Dropout(dropout_rate)(conv9)\n",
        "\n",
        "    # 출력\n",
        "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=conv10)\n",
        "\n",
        "    return model\n",
        "\n",
        "class_weight = {0: 1, 1: 30}\n",
        "\n",
        "unet = unet_model()\n",
        "unet.compile(optimizer='adam',\n",
        "             loss=BinaryCrossentropy(from_logits=False),\n",
        "             metrics=['accuracy', Precision(), Recall(), AUC()])"
      ],
      "metadata": {
        "id": "czsAGKmjHyHt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                               patience=10,\n",
        "                               verbose=1,\n",
        "                               mode='min',\n",
        "                               restore_best_weights=True)\n",
        "\n",
        "checkpoint = ModelCheckpoint('colab/model/dog_unet_model_{epoch:02d}.h5',\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=False,\n",
        "                             mode='min',\n",
        "                             save_freq='epoch')"
      ],
      "metadata": {
        "id": "qX6yh944lxPj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = unet.fit(train_dataset, epochs=30, steps_per_epoch=train_size // batch_size,\n",
        "                   validation_data=val_dataset, validation_steps=val_size // batch_size,\n",
        "                   class_weight=class_weight,\n",
        "                   callbacks=[checkpoint, early_stopping],\n",
        "                   verbose=1)"
      ],
      "metadata": {
        "id": "4IS7D1lXoeDF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5caf5fe6-91d3-4a63-d4e4-a6834a2c979d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1d993d93947f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = unet.fit(train_dataset, epochs=30, steps_per_epoch=train_size // batch_size,\n\u001b[0m\u001b[1;32m      2\u001b[0m                    \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    verbose=1)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node model/concatenate_2/concat defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-19-1d993d93947f>\", line 1, in <cell line: 1>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/merging/base_merge.py\", line 196, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/merging/concatenate.py\", line 134, in _merge_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 3580, in concatenate\n\nOOM when allocating tensor with shape[64,128,128,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/concatenate_2/concat}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_10151]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(history.history.keys()))"
      ],
      "metadata": {
        "id": "HPtFjDs6CYar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "# 훈련 및 검증 손실 그래프\n",
        "plt.subplot(1, 5, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# 훈련 및 검증 정확도 그래프\n",
        "plt.subplot(1, 5, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# 훈련 및 검증 정밀도 그래프\n",
        "plt.subplot(1, 5, 3)\n",
        "plt.plot(history.history['precision_4'], label='Training Precision')\n",
        "plt.plot(history.history['val_precision_4'], label='Validation Precision')\n",
        "plt.title('Training and Validation Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "\n",
        "# 훈련 및 검증 재현율 그래프\n",
        "plt.subplot(1, 5, 4)\n",
        "plt.plot(history.history['recall_4'], label='Training Recall')\n",
        "plt.plot(history.history['val_recall_4'], label='Validation Recall')\n",
        "plt.title('Training and Validation Recall')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 5, 5)\n",
        "plt.plot(history.history['auc_4'], label='Training AUC')\n",
        "plt.plot(history.history['val_auc_4'], label='Validation AUC')\n",
        "plt.title('Training and Validation AUC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FHSwVUptlxUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6AscDIyXlxk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3zj0-5iTlxnd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}